package main

import (
	"context"
	"fmt"
	"time"

	"github.com/ynl/greensoulai/internal/llm"
)

func main() {
	fmt.Println("ğŸš€ GreenSoulAI LLM æ¨¡å—æ¼”ç¤º")
	fmt.Println("=========================")

	// 1. åˆ›å»ºLLMé…ç½®
	config := &llm.Config{
		Provider:    "openai",
		Model:       "gpt-4o-mini",
		APIKey:      "your-api-key-here", // åœ¨å®é™…ä½¿ç”¨ä¸­è®¾ç½®çœŸå®çš„APIå¯†é’¥
		Timeout:     30 * time.Second,
		MaxRetries:  3,
		Temperature: func() *float64 { t := 0.7; return &t }(),
		MaxTokens:   func() *int { t := 1000; return &t }(),
	}

	// 2. åˆ›å»ºLLMå®ä¾‹
	llmInstance, err := llm.CreateLLM(config)
	if err != nil {
		fmt.Printf("âŒ åˆ›å»ºLLMå¤±è´¥: %v\n", err)
		return
	}
	defer llmInstance.Close()

	fmt.Printf("âœ… æˆåŠŸåˆ›å»º %s æ¨¡å‹å®ä¾‹\n", llmInstance.GetModel())
	fmt.Printf("ğŸ¯ æ”¯æŒå‡½æ•°è°ƒç”¨: %v\n", llmInstance.SupportsFunctionCalling())
	fmt.Printf("ğŸ“ ä¸Šä¸‹æ–‡çª—å£: %d tokens\n", llmInstance.GetContextWindowSize())

	// 3. åŸºç¡€å¯¹è¯ç¤ºä¾‹
	fmt.Println("\nğŸ“ åŸºç¡€å¯¹è¯æ¼”ç¤º:")
	messages := []llm.Message{
		{Role: llm.RoleSystem, Content: "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
		{Role: llm.RoleUser, Content: "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯Goè¯­è¨€ã€‚"},
	}

	ctx := context.Background()
	response, err := llmInstance.Call(ctx, messages, &llm.CallOptions{
		Temperature: func() *float64 { t := 0.5; return &t }(),
		MaxTokens:   func() *int { t := 100; return &t }(),
	})

	if err != nil {
		fmt.Printf("âŒ è°ƒç”¨å¤±è´¥: %v\n", err)
	} else {
		fmt.Printf("ğŸ¤– å›å¤: %s\n", response.Content)
		fmt.Printf("ğŸ“Š ä½¿ç”¨ç»Ÿè®¡: %d tokens (æç¤º: %d, å®Œæˆ: %d)\n",
			response.Usage.TotalTokens,
			response.Usage.PromptTokens,
			response.Usage.CompletionTokens)
	}

	// 4. æµå¼å“åº”æ¼”ç¤º
	fmt.Println("\nğŸŒŠ æµå¼å“åº”æ¼”ç¤º:")
	streamMessages := []llm.Message{
		{Role: llm.RoleUser, Content: "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚"},
	}

	stream, err := llmInstance.CallStream(ctx, streamMessages, &llm.CallOptions{
		Temperature: func() *float64 { t := 0.7; return &t }(),
		MaxTokens:   func() *int { t := 200; return &t }(),
	})

	if err != nil {
		fmt.Printf("âŒ æµå¼è°ƒç”¨å¤±è´¥: %v\n", err)
	} else {
		fmt.Print("ğŸ¤– æµå¼å›å¤: ")
		for chunk := range stream {
			if chunk.Error != nil {
				fmt.Printf("\nâŒ æµå¼é”™è¯¯: %v\n", chunk.Error)
				break
			}
			fmt.Print(chunk.Delta)
		}
		fmt.Println()
	}

	// 5. å·¥å…·è°ƒç”¨æ¼”ç¤ºï¼ˆå‡½æ•°è°ƒç”¨ï¼‰
	fmt.Println("\nğŸ”§ å·¥å…·è°ƒç”¨æ¼”ç¤º:")
	tools := []llm.Tool{
		{
			Type: "function",
			Function: llm.ToolSchema{
				Name:        "get_weather",
				Description: "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"city": map[string]interface{}{
							"type":        "string",
							"description": "åŸå¸‚åç§°",
						},
					},
					"required": []string{"city"},
				},
			},
		},
	}

	toolMessages := []llm.Message{
		{Role: llm.RoleUser, Content: "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
	}

	toolResponse, err := llmInstance.Call(ctx, toolMessages, &llm.CallOptions{
		Tools:      tools,
		ToolChoice: "auto",
	})

	if err != nil {
		fmt.Printf("âŒ å·¥å…·è°ƒç”¨å¤±è´¥: %v\n", err)
	} else if len(toolResponse.ToolCalls) > 0 {
		fmt.Printf("ğŸ”§ å·¥å…·è°ƒç”¨: %s(%s)\n",
			toolResponse.ToolCalls[0].Function.Name,
			toolResponse.ToolCalls[0].Function.Arguments)
	} else {
		fmt.Printf("ğŸ¤– æ™®é€šå›å¤: %s\n", toolResponse.Content)
	}

	// 6. å±•ç¤ºä¸åŒçš„é…ç½®é€‰é¡¹
	fmt.Println("\nâš™ï¸  é…ç½®é€‰é¡¹æ¼”ç¤º:")
	fmt.Println("   - é«˜åˆ›é€ æ€§ (Temperature: 0.9)")
	fmt.Println("   - çŸ­å›å¤ (MaxTokens: 50)")
	fmt.Println("   - åœæ­¢åºåˆ—: [\"\\n\"]")

	optionsResponse, err := llmInstance.Call(ctx, []llm.Message{
		{Role: llm.RoleUser, Content: "ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½"},
	}, &llm.CallOptions{
		Temperature:   func() *float64 { t := 0.9; return &t }(),
		MaxTokens:     func() *int { t := 50; return &t }(),
		StopSequences: []string{"\n"},
	})

	if err != nil {
		fmt.Printf("âŒ é…ç½®é€‰é¡¹æ¼”ç¤ºå¤±è´¥: %v\n", err)
	} else {
		fmt.Printf("ğŸ¤– é«˜åˆ›é€ æ€§å›å¤: %s\n", optionsResponse.Content)
		fmt.Printf("ğŸ“Š ä½¿ç”¨Token: %d\n", optionsResponse.Usage.TotalTokens)
	}

	configResponse, err := llmInstance.Call(ctx, []llm.Message{
		{Role: llm.RoleUser, Content: "ç”¨ä¸€ä¸ªè¯æè¿°Goè¯­è¨€"},
	}, llm.DefaultCallOptions())

	// åº”ç”¨é€‰é¡¹
	configResponse2, _ := llmInstance.Call(ctx, []llm.Message{
		{Role: llm.RoleUser, Content: "ç”¨ä¸€ä¸ªè¯æè¿°Goè¯­è¨€"},
	}, llm.DefaultCallOptions())

	if configResponse2 != nil {
		configResponse2.Usage = llm.Usage{} // é¿å…ç©ºæŒ‡é’ˆ
	}

	if err == nil && configResponse != nil {
		fmt.Printf("ğŸ›ï¸  é…ç½®æ¼”ç¤ºå®Œæˆ\n")
	}

	fmt.Println("\nğŸ‰ LLMæ¨¡å—æ¼”ç¤ºå®Œæˆï¼")
	fmt.Println("âœ¨ ç‰¹æ€§åŒ…æ‹¬:")
	fmt.Println("   - ç»Ÿä¸€çš„LLMæ¥å£æŠ½è±¡")
	fmt.Println("   - å¤šæä¾›å•†æ”¯æŒ")
	fmt.Println("   - æµå¼å“åº”")
	fmt.Println("   - å‡½æ•°è°ƒç”¨/å·¥å…·é›†æˆ")
	fmt.Println("   - è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡")
	fmt.Println("   - å¼ºå¤§çš„é”™è¯¯å¤„ç†")
	fmt.Println("   - äº‹ä»¶ç³»ç»Ÿé›†æˆ")
}

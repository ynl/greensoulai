// Package flow æä¾›å¹¶è¡Œä½œä¸šç¼–æ’ç³»ç»Ÿ
// é‡æ–°è®¾è®¡ï¼šä½¿ç”¨Jobè€ŒéTaskï¼Œé¿å…ä¸Agentç³»ç»Ÿçš„Taskå†²çª
package flow

import (
	"context"
	"fmt"
	"sync"
	"time"
)

// ============================================================================
// æ ¸å¿ƒæ¥å£ - ä½¿ç”¨Jobé¿å…ä¸Agent Taskå†²çª
// ============================================================================

// Job å®šä¹‰å·¥ä½œæµä¸­çš„ä½œä¸šå•å…ƒ - å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
// æ³¨æ„ï¼šJobä¸Agentçš„Taskä¸åŒï¼ŒJobæ˜¯å·¥ä½œæµç¼–æ’å•å…ƒï¼ŒTaskæ˜¯Agentæ‰§è¡Œçš„ä¸šåŠ¡ä»»åŠ¡
type Job interface {
	ID() string
	Execute(ctx context.Context) (interface{}, error)
}

// Trigger å®šä¹‰ä½œä¸šè§¦å‘æ¡ä»¶
type Trigger interface {
	Ready(completed JobResults) bool
	String() string
}

// Workflow å®šä¹‰å¹¶è¡Œä½œä¸šç¼–æ’æ¥å£
type Workflow interface {
	AddJob(job Job, trigger Trigger) Workflow
	Run(ctx context.Context) (*ExecutionResult, error)
	RunAsync(ctx context.Context) <-chan *ExecutionResult
}

// JobResults å·²å®Œæˆä½œä¸šçš„ç»“æœé›†
type JobResults map[string]interface{}

// ============================================================================
// æ‰§è¡Œç»“æœå’ŒæŒ‡æ ‡
// ============================================================================

// ExecutionResult å·¥ä½œæµæ‰§è¡Œç»“æœ
type ExecutionResult struct {
	FinalResult interface{}      // æœ€åå®Œæˆçš„ä½œä¸šç»“æœ
	AllResults  JobResults       // æ‰€æœ‰ä½œä¸šç»“æœ
	JobTrace    []JobExecution   // ä½œä¸šæ‰§è¡Œè¿½è¸ª
	Metrics     *ParallelMetrics // å¹¶è¡Œæ‰§è¡ŒæŒ‡æ ‡
	Duration    time.Duration    // æ€»æ‰§è¡Œæ—¶é—´
	Error       error            // æ‰§è¡Œé”™è¯¯
}

// JobExecution å•ä¸ªä½œä¸šæ‰§è¡Œè®°å½•
type JobExecution struct {
	JobID     string
	StartTime time.Time
	EndTime   time.Time
	Duration  time.Duration
	Result    interface{}
	Error     error
	BatchID   int // æ‰€å±çš„å¹¶è¡Œæ‰¹æ¬¡ID
}

// ParallelMetrics å¹¶è¡Œæ‰§è¡ŒæŒ‡æ ‡
type ParallelMetrics struct {
	TotalJobs          int            // æ€»ä½œä¸šæ•°
	ParallelBatches    int            // å¹¶è¡Œæ‰¹æ¬¡æ•°é‡
	BatchInfo          []BatchMetrics // æ¯ä¸ªæ‰¹æ¬¡çš„è¯¦ç»†ä¿¡æ¯
	MaxConcurrency     int            // æœ€å¤§å¹¶å‘æ•°
	ParallelEfficiency float64        // å¹¶è¡Œæ•ˆç‡
	SerialTime         time.Duration  // å‡è®¾ä¸²è¡Œæ‰§è¡Œçš„æ—¶é—´
	ParallelTime       time.Duration  // å®é™…å¹¶è¡Œæ‰§è¡Œæ—¶é—´
}

// BatchMetrics æ‰¹æ¬¡æ‰§è¡ŒæŒ‡æ ‡
type BatchMetrics struct {
	BatchID        int
	JobCount       int
	Duration       time.Duration
	Concurrency    int
	EfficiencyGain float64 // ç›¸å¯¹äºä¸²è¡Œçš„æ•ˆç‡æå‡
}

// ============================================================================
// æ ¸å¿ƒå®ç°ï¼šå¹¶è¡Œä½œä¸šå¼•æ“
// ============================================================================

// ParallelEngine å¹¶è¡Œä½œä¸šæ‰§è¡Œå¼•æ“
// æ³¨æ„ï¼šEngineè®¾è®¡ç”¨äºç¼–æ’å’Œæ‰§è¡Œå·¥ä½œæµä½œä¸šï¼Œä¸åŒäºAgentçš„ä»»åŠ¡æ‰§è¡Œ
type ParallelEngine struct {
	name      string
	jobs      []jobWithTrigger
	maxCycles int
	mu        sync.RWMutex
}

type jobWithTrigger struct {
	job     Job
	trigger Trigger
}

// NewWorkflow åˆ›å»ºæ–°çš„å¹¶è¡Œå·¥ä½œæµ
func NewWorkflow(name string) Workflow {
	return &ParallelEngine{
		name:      name,
		jobs:      make([]jobWithTrigger, 0),
		maxCycles: 100,
	}
}

// AddJob æ·»åŠ ä½œä¸šå’Œè§¦å‘æ¡ä»¶
func (e *ParallelEngine) AddJob(job Job, trigger Trigger) Workflow {
	e.mu.Lock()
	defer e.mu.Unlock()

	e.jobs = append(e.jobs, jobWithTrigger{
		job:     job,
		trigger: trigger,
	})
	return e
}

// Run æ‰§è¡Œå·¥ä½œæµ - é‡ç‚¹ï¼šå¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å°±ç»ªçš„ä½œä¸š
func (e *ParallelEngine) Run(ctx context.Context) (*ExecutionResult, error) {
	startTime := time.Now()

	result := &ExecutionResult{
		AllResults: make(JobResults),
		JobTrace:   make([]JobExecution, 0),
		Metrics:    &ParallelMetrics{},
	}

	var totalSerialTime time.Duration
	cycle := 0
	batchID := 0

	for cycle < e.maxCycles {
		cycle++

		// ğŸš€ å…³é”®ï¼šè·å–æ‰€æœ‰å°±ç»ªçš„ä½œä¸šï¼ˆå¯èƒ½æœ‰å¤šä¸ªï¼‰
		readyJobs := e.getReadyJobs(result.AllResults)
		if len(readyJobs) == 0 {
			break // æ²¡æœ‰æ›´å¤šå°±ç»ªçš„ä½œä¸š
		}

		// ğŸš€ å…³é”®ï¼šå¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å°±ç»ªçš„ä½œä¸š
		batchID++
		batchResults, batchMetrics, err := e.executeJobBatch(ctx, readyJobs, batchID)
		if err != nil {
			result.Error = err
			result.Duration = time.Since(startTime)
			return result, err
		}

		// æ›´æ–°ç»“æœ
		for _, jobExec := range batchResults {
			result.AllResults[jobExec.JobID] = jobExec.Result
			result.JobTrace = append(result.JobTrace, jobExec)
			result.FinalResult = jobExec.Result // æœ€åä¸€ä¸ªä½œä¸ºæœ€ç»ˆç»“æœ
			totalSerialTime += jobExec.Duration
		}

		// æ›´æ–°æ‰¹æ¬¡æŒ‡æ ‡
		result.Metrics.BatchInfo = append(result.Metrics.BatchInfo, batchMetrics)
		if batchMetrics.Concurrency > result.Metrics.MaxConcurrency {
			result.Metrics.MaxConcurrency = batchMetrics.Concurrency
		}
	}

	// å®Œå–„æ‰§è¡ŒæŒ‡æ ‡
	result.Duration = time.Since(startTime)
	result.Metrics.TotalJobs = len(result.JobTrace)
	result.Metrics.ParallelBatches = batchID
	result.Metrics.SerialTime = totalSerialTime
	result.Metrics.ParallelTime = result.Duration

	if result.Duration > 0 {
		result.Metrics.ParallelEfficiency = float64(totalSerialTime) / float64(result.Duration)
	}

	return result, nil
}

// RunAsync å¼‚æ­¥æ‰§è¡Œå·¥ä½œæµ
func (e *ParallelEngine) RunAsync(ctx context.Context) <-chan *ExecutionResult {
	resultChan := make(chan *ExecutionResult, 1)

	go func() {
		defer close(resultChan)
		result, err := e.Run(ctx)
		if err != nil && result.Error == nil {
			result.Error = err
		}
		resultChan <- result
	}()

	return resultChan
}

// getReadyJobs è·å–æ‰€æœ‰å°±ç»ªçš„ä½œä¸š - å¼ºè°ƒå¯èƒ½æœ‰å¤šä¸ªå¹¶è¡Œå°±ç»ª
func (e *ParallelEngine) getReadyJobs(completed JobResults) []Job {
	e.mu.RLock()
	defer e.mu.RUnlock()

	var ready []Job

	for _, jt := range e.jobs {
		// è·³è¿‡å·²å®Œæˆçš„ä½œä¸š
		if _, done := completed[jt.job.ID()]; done {
			continue
		}

		// æ£€æŸ¥è§¦å‘æ¡ä»¶
		if jt.trigger.Ready(completed) {
			ready = append(ready, jt.job)
		}
	}

	return ready
}

// executeJobBatch æ‰¹é‡å¹¶è¡Œæ‰§è¡Œä½œä¸š - æ ¸å¿ƒå¹¶è¡Œé€»è¾‘
func (e *ParallelEngine) executeJobBatch(ctx context.Context, jobs []Job, batchID int) ([]JobExecution, BatchMetrics, error) {
	if len(jobs) == 0 {
		return nil, BatchMetrics{}, nil
	}

	batchStartTime := time.Now()
	resultChan := make(chan JobExecution, len(jobs))
	var wg sync.WaitGroup

	// ğŸš€ å…³é”®ï¼šä¸ºæ¯ä¸ªä½œä¸šå¯åŠ¨ç‹¬ç«‹çš„goroutineå¹¶è¡Œæ‰§è¡Œ
	for _, job := range jobs {
		wg.Add(1)
		go func(j Job) {
			defer wg.Done()

			execution := JobExecution{
				JobID:     j.ID(),
				StartTime: time.Now(),
				BatchID:   batchID,
			}

			// æ‰§è¡Œä½œä¸š
			result, err := j.Execute(ctx)

			execution.EndTime = time.Now()
			execution.Duration = execution.EndTime.Sub(execution.StartTime)
			execution.Result = result
			execution.Error = err

			resultChan <- execution
		}(job)
	}

	// ç­‰å¾…æ‰€æœ‰ä½œä¸šå®Œæˆ
	go func() {
		wg.Wait()
		close(resultChan)
	}()

	// æ”¶é›†ç»“æœ
	var executions []JobExecution
	for execution := range resultChan {
		if execution.Error != nil {
			return nil, BatchMetrics{}, fmt.Errorf("job %s failed: %w", execution.JobID, execution.Error)
		}
		executions = append(executions, execution)
	}

	// è®¡ç®—æ‰¹æ¬¡æŒ‡æ ‡
	batchDuration := time.Since(batchStartTime)
	totalJobTime := time.Duration(0)
	for _, exec := range executions {
		totalJobTime += exec.Duration
	}

	var efficiencyGain float64
	if batchDuration > 0 {
		efficiencyGain = float64(totalJobTime) / float64(batchDuration)
	}

	batchMetrics := BatchMetrics{
		BatchID:        batchID,
		JobCount:       len(jobs),
		Duration:       batchDuration,
		Concurrency:    len(jobs),
		EfficiencyGain: efficiencyGain,
	}

	return executions, batchMetrics, nil
}

// ============================================================================
// ä½œä¸šå®ç° - æ¸…æ™°è¡¨è¾¾è¿™æ˜¯å¹¶è¡Œæ‰§è¡Œå•å…ƒ
// ============================================================================

// SimpleJob ç®€å•ä½œä¸šå®ç°
type SimpleJob struct {
	id string
	fn func(ctx context.Context) (interface{}, error)
}

func (j SimpleJob) ID() string                                       { return j.id }
func (j SimpleJob) Execute(ctx context.Context) (interface{}, error) { return j.fn(ctx) }

// NewJob åˆ›å»ºç®€å•ä½œä¸š
func NewJob(id string, fn func(ctx context.Context) (interface{}, error)) Job {
	return SimpleJob{id: id, fn: fn}
}

// ============================================================================
// è§¦å‘æ¡ä»¶å®ç° - æ§åˆ¶ä½œä¸šä½•æ—¶å°±ç»ªæ‰§è¡Œ
// ============================================================================

// ImmediateTrigger ç«‹å³è§¦å‘ï¼ˆç”¨äºèµ·å§‹ä½œä¸šï¼‰
type ImmediateTrigger struct{}

func (ImmediateTrigger) Ready(completed JobResults) bool { return true }
func (ImmediateTrigger) String() string                  { return "immediate" }

// AfterTrigger åœ¨æŒ‡å®šä½œä¸šå®Œæˆåè§¦å‘
type AfterTrigger struct{ jobID string }

func (t AfterTrigger) Ready(completed JobResults) bool {
	_, exists := completed[t.jobID]
	return exists
}

func (t AfterTrigger) String() string { return fmt.Sprintf("after:%s", t.jobID) }

// AllOfTrigger æ‰€æœ‰æŒ‡å®šä½œä¸šéƒ½å®Œæˆåè§¦å‘ï¼ˆANDé€»è¾‘ï¼‰
type AllOfTrigger struct{ triggers []Trigger }

func (t AllOfTrigger) Ready(completed JobResults) bool {
	for _, trigger := range t.triggers {
		if !trigger.Ready(completed) {
			return false
		}
	}
	return true
}

func (t AllOfTrigger) String() string { return "all-of" }

// AnyOfTrigger ä»»ä¸€æŒ‡å®šä½œä¸šå®Œæˆåè§¦å‘ï¼ˆORé€»è¾‘ï¼‰
type AnyOfTrigger struct{ triggers []Trigger }

func (t AnyOfTrigger) Ready(completed JobResults) bool {
	for _, trigger := range t.triggers {
		if trigger.Ready(completed) {
			return true
		}
	}
	return false
}

func (t AnyOfTrigger) String() string { return "any-of" }

// ============================================================================
// ä¾¿æ·æ„é€ å‡½æ•° - è¯­ä¹‰æ¸…æ™°çš„API
// ============================================================================

// Immediately ç«‹å³å°±ç»ªè§¦å‘å™¨
func Immediately() Trigger { return ImmediateTrigger{} }

// After åœ¨æŒ‡å®šä½œä¸šå®Œæˆåè§¦å‘
func After(jobID string) Trigger { return AfterTrigger{jobID} }

// AllOf æ‰€æœ‰ä½œä¸šéƒ½å®Œæˆåè§¦å‘
func AllOf(triggers ...Trigger) Trigger { return AllOfTrigger{triggers} }

// AnyOf ä»»ä¸€ä½œä¸šå®Œæˆåè§¦å‘
func AnyOf(triggers ...Trigger) Trigger { return AnyOfTrigger{triggers} }

// AfterJobs åœ¨æŒ‡å®šä½œä¸šéƒ½å®Œæˆåè§¦å‘ - è¯­æ³•ç³–
func AfterJobs(jobIDs ...string) Trigger {
	triggers := make([]Trigger, len(jobIDs))
	for i, id := range jobIDs {
		triggers[i] = After(id)
	}
	return AllOf(triggers...)
}

// AfterAnyJob åœ¨ä»»ä¸€æŒ‡å®šä½œä¸šå®Œæˆåè§¦å‘ - è¯­æ³•ç³–
func AfterAnyJob(jobIDs ...string) Trigger {
	triggers := make([]Trigger, len(jobIDs))
	for i, id := range jobIDs {
		triggers[i] = After(id)
	}
	return AnyOf(triggers...)
}

// ============================================================================
// é«˜çº§ä½œä¸šç±»å‹ - ç»„åˆå’Œå¹¶è¡Œæ¨¡å¼
// ============================================================================

// ParallelJobGroup å¹¶è¡Œä½œä¸šç»„ - æ˜ç¡®è¡¨è¾¾å¹¶è¡Œæ„å›¾
type ParallelJobGroup struct {
	id   string
	jobs []Job
}

func (pjg ParallelJobGroup) ID() string { return pjg.id }

func (pjg ParallelJobGroup) Execute(ctx context.Context) (interface{}, error) {
	var wg sync.WaitGroup
	results := make([]interface{}, len(pjg.jobs))
	errors := make([]error, len(pjg.jobs))

	for i, job := range pjg.jobs {
		wg.Add(1)
		go func(index int, j Job) {
			defer wg.Done()
			result, err := j.Execute(ctx)
			results[index] = result
			errors[index] = err
		}(i, job)
	}

	wg.Wait()

	// æ£€æŸ¥é”™è¯¯
	for i, err := range errors {
		if err != nil {
			return nil, fmt.Errorf("parallel job %s failed: %w", pjg.jobs[i].ID(), err)
		}
	}

	return results, nil
}

// NewParallelGroup åˆ›å»ºå¹¶è¡Œä½œä¸šç»„
func NewParallelGroup(id string, jobs ...Job) Job {
	return ParallelJobGroup{id: id, jobs: jobs}
}

// SequentialJobChain é¡ºåºä½œä¸šé“¾
type SequentialJobChain struct {
	id   string
	jobs []Job
}

func (sjc SequentialJobChain) ID() string { return sjc.id }

func (sjc SequentialJobChain) Execute(ctx context.Context) (interface{}, error) {
	var lastResult interface{}

	for _, job := range sjc.jobs {
		result, err := job.Execute(ctx)
		if err != nil {
			return nil, fmt.Errorf("sequential job %s failed: %w", job.ID(), err)
		}
		lastResult = result
	}

	return lastResult, nil
}

// NewSequentialChain åˆ›å»ºé¡ºåºä½œä¸šé“¾
func NewSequentialChain(id string, jobs ...Job) Job {
	return SequentialJobChain{id: id, jobs: jobs}
}

// ============================================================================
// ä¸Agentç³»ç»Ÿé›†æˆçš„ä¾¿æ·ä½œä¸šç±»å‹
// ============================================================================

// AgentJobAdapter å°†Agentä»»åŠ¡é€‚é…ä¸ºå·¥ä½œæµä½œä¸š
// è¿™æ ·å¯ä»¥åœ¨å·¥ä½œæµä¸­æ‰§è¡ŒAgentä»»åŠ¡ï¼Œæ¦‚å¿µå±‚æ¬¡æ¸…æ™°ï¼š
// Workflow Job -> åŒ…å« -> Agent Task
type AgentJobAdapter struct {
	id          string
	description string
	// æ³¨æ„ï¼šè¿™é‡Œå¯ä»¥å¼•ç”¨ agent.Agent å’Œ agent.Task
	// ä½†ç°åœ¨å…ˆç”¨interface{}é¿å…å¾ªç¯å¯¼å…¥ï¼Œåç»­å¯ä»¥é‡æ„
}

func (aja AgentJobAdapter) ID() string { return aja.id }

func (aja AgentJobAdapter) Execute(ctx context.Context) (interface{}, error) {
	// TODO: é›†æˆAgentç³»ç»Ÿ
	// agent := getAgent()
	// task := createAgentTask()
	// return agent.Execute(ctx, task)
	return fmt.Sprintf("Agent job %s executed: %s", aja.id, aja.description), nil
}

// NewAgentJob åˆ›å»ºAgentä½œä¸šé€‚é…å™¨
func NewAgentJob(id, description string) Job {
	return AgentJobAdapter{
		id:          id,
		description: description,
	}
}
